{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Ð—Ð°Ð´Ð°Ð½Ð¸Ðµ 1\n",
    "\n",
    "(**NB.** Ð´Ð»Ñ Ð·Ð°Ð¿ÑƒÑÐºÐ° Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð¾Ð² ÐºÐ¾Ð´Ð° Ð½ÑƒÐ¶ÐµÐ½ Python Ð²ÐµÑ€ÑÐ¸Ð¸ Ð½Ðµ Ð½Ð¸Ð¶Ðµ **3.10**, Ð´Ð¾Ð¿ÑƒÑÐºÐ°ÐµÑ‚ÑÑ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´Ñ€ÑƒÐ³Ð¸Ñ… Ð²ÐµÑ€ÑÐ¸Ð¹, Ð² ÑÑ‚Ð¾Ð¼ ÑÐ»ÑƒÑ‡Ð°Ðµ Ð½ÑƒÐ¶Ð½Ð¾ ÑÐ°Ð¼Ð¾ÑÑ‚Ð¾ÑÑ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¸Ð·Ð±Ð°Ð²Ð¸Ñ‚ÑŒÑÑ Ð¾Ñ‚ ÐºÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¸ `match`).\n",
    "\n",
    "Ð•ÑÑ‚ÑŒ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ ÐºÐ¾Ð´ Ð´Ð»Ñ [Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð´Ð¸Ñ„Ñ„ÐµÑ€ÐµÐ½Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ](https://en.wikipedia.org/wiki/Automatic_differentiation), Ð² ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð¼ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‚ÑÑ Ð¾ÑÐ¾Ð±ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹ Ñ‚Ð¸Ð¿Ð¾Ð² ÑÐ·Ñ‹ÐºÐ° `Python`: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Union, Callable\n",
    "from numbers import Number\n",
    "\n",
    "@dataclass(slots=True) #improved\n",
    "class Dual:\n",
    "    value: float\n",
    "    d: float\n",
    "\n",
    "    def __add__(self, other: Union[\"Dual\", Number]) -> \"Dual\":\n",
    "         match other:\n",
    "            case Dual(o_value, o_d):\n",
    "                return Dual(self.value + o_value, self.d + o_d)\n",
    "            case Number():\n",
    "                return Dual(float(other) + self.value, self.d)\n",
    "\n",
    "    def __mul__(self, other: Union[\"Dual\", Number]) -> \"Dual\":\n",
    "         match other:\n",
    "            case Dual(o_value, o_d):\n",
    "                return Dual(self.value * o_value, self.value * o_d + self.d * o_value)\n",
    "            case Number():\n",
    "                return Dual(float(other) * self.value, float(other) * self.d)    \n",
    "\n",
    "    __rmul__ = __mul__  # https://docs.python.org/3/reference/datamodel.html#object.__mul__\n",
    "    __radd__ = __add__  # https://docs.python.org/3/reference/datamodel.html#object.__radd__\n",
    " \n",
    "\n",
    "def diff(func: Callable[[float], float]) -> Callable[[float], float]:\n",
    "    return lambda x: func(Dual(x, 1.0)).d "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "ÐŸÐ¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÑŽÑ‚ÑÑ Ð´Ð²Ðµ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸ - ÑÐ»Ð¾Ð¶ÐµÐ½Ð¸Ðµ Ð¸ ÑƒÐ¼Ð½Ð¾Ð¶ÐµÐ½Ð¸Ðµ. ÐŸÑ€Ð¸Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ð¼Ð¾Ð¶Ð½Ð¾ Ñ‚Ð°Ðº:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ, ÐºÐ¾Ñ‚Ð¾Ñ€ÑƒÑŽ Ð±ÑƒÐ´ÐµÐ¼ Ð´Ð¸Ñ„Ñ„ÐµÑ€ÐµÐ½Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ\n",
    "def f(x: float) -> float:\n",
    "    return 5 * x * x + 2 * x + 2\n",
    "\n",
    "f_diff = diff(f)\n",
    "\n",
    "# Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð½Ð¾Ð¹ Ð² Ñ‚Ð¾Ñ‡ÐºÐµ x = 2\n",
    "f_diff(2)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "22.0"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ð—Ð°Ð´Ð°Ð½Ð¸Ðµ 1.1 (5 Ð±Ð°Ð»Ð»Ð¾Ð²)\n",
    "\n",
    "ÐšÐ°ÐºÐ¸Ðµ Ð½ÐµÐ´Ð¾ÑÑ‚Ð°Ñ‚ÐºÐ¸ Ð²Ñ‹ Ð²Ð¸Ð´Ð¸Ñ‚Ðµ Ð² Ð´Ð°Ð½Ð½Ð¾Ð¹ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸? Ð ÐµÐ°Ð»Ð¸Ð·ÑƒÐ¹Ñ‚Ðµ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÑƒ (Ð¿Ð¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ ÑÐ°Ð¼Ð¾ÑÑ‚Ð¾ÑÑ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¸Ð»Ð¸ Ð¼Ð¾Ð´Ð¸Ñ„Ð¸Ñ†Ð¸Ñ€ÑƒÑ Ð¿Ñ€Ð¸Ð²ÐµÐ´ÐµÐ½Ð½Ñ‹Ð¹ ÐºÐ¾Ð´):\n",
    "- [ÑƒÐ½Ð°Ñ€Ð½Ñ‹Ñ… Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹](https://docs.python.org/3/reference/datamodel.html#object.__neg__) \n",
    "- Ð´ÐµÐ»ÐµÐ½Ð¸Ñ\n",
    "- Ð²Ð¾Ð·Ð²ÐµÐ´ÐµÐ½Ð¸Ñ Ð² ÑÑ‚ÐµÐ¿ÐµÐ½ÑŒ\n",
    "\n",
    "ÐšÐ°ÐºÐ¸Ð¼ Ð¾Ð±Ñ€Ð°Ð·Ð¾Ð¼ Ð¼Ð¾Ð¶Ð½Ð¾ Ð¿Ñ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð¾ÑÑ‚ÑŒ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ?  Ð ÐµÐ°Ð»Ð¸Ð·ÑƒÐ¹Ñ‚Ðµ Ð´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ñ‹Ð¹, Ð¿Ð¾ Ð²Ð°ÑˆÐµÐ¼Ñƒ Ð¼Ð½ÐµÐ½Ð¸ÑŽ, Ð½Ð°Ð±Ð¾Ñ€ Ñ‚ÐµÑÑ‚Ð¾Ð²."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ð—Ð°Ð´Ð°Ð½Ð¸Ðµ 1.2 (7 Ð±Ð°Ð»Ð»Ð¾Ð²)\n",
    "ÐŸÑ€Ð¸Ð´ÑƒÐ¼Ð°Ð¹Ñ‚Ðµ ÑÐ¿Ð¾ÑÐ¾Ð± Ð¸ Ñ€ÐµÐ°Ð»Ð¸Ð·ÑƒÐ¹Ñ‚Ðµ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÑƒ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¹:\n",
    "- `exp()`\n",
    "- `cos()`\n",
    "- `sin()`\n",
    "- `log()`\n",
    "\n",
    "Ð”Ð¾Ð±Ð°Ð²ÑŒÑ‚Ðµ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ Ñ‚ÐµÑÑ‚Ñ‹"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ð—Ð°Ð´Ð°Ð½Ð¸Ðµ 1.3 (3 Ð±Ð°Ð»Ð»Ð°)\n",
    "\n",
    "Ð’Ð¾ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚ÐµÑÑŒ Ð¼ÐµÑ‚Ð¾Ð´Ð°Ð¼Ð¸ **Ñ‡Ð¸ÑÐ»ÐµÐ½Ð½Ð¾Ð³Ð¾** Ð´Ð¸Ñ„Ñ„ÐµÑ€ÐµÐ½Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð´Ð»Ñ \"Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸\" Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ ÐºÐ¾Ð´Ð° Ð½Ð° Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð°Ñ…. ÐÐ°Ð¿Ñ€Ð¸Ð¼ÐµÑ€,  Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐµ `scipy` ÐµÑÑ‚ÑŒ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ `derivative`. Ð˜Ð»Ð¸ Ñ€ÐµÐ°Ð»Ð¸Ð·ÑƒÐ¹Ñ‚Ðµ ÐºÐ°ÐºÐ¾Ð¹-Ð½Ð¸Ð±ÑƒÐ´ÑŒ Ð¼ÐµÑ‚Ð¾Ð´ Ñ‡Ð¸ÑÐ»ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð´Ð¸Ñ„Ñ„ÐµÑ€ÐµÐ½Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ ÑÐ°Ð¼Ð¾ÑÑ‚Ð¾ÑÑ‚ÐµÐ»ÑŒÐ½Ð¾ (**+5 Ð±Ð°Ð»Ð»Ð¾Ð²**)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from scipy.misc import derivative\n",
    "\n",
    "def f(x: float) -> float:\n",
    "    return 5 * x * x + 2 * x + 2\n",
    "\n",
    "derivative(f, 2.)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "22.0"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ð—Ð°Ð´Ð°Ð½Ð¸Ðµ 1.4 (10 Ð±Ð°Ð»Ð»Ð¾Ð²)\n",
    "\n",
    "ÐÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ ÑÐ¸ÑÑ‚ÐµÐ¼Ñƒ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼Ð° Ð´Ð¸Ñ„Ñ„ÐµÑ€ÐµÐ½Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð² ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÐµÐ¼ Ð²Ð¸Ð´Ðµ:\n",
    "- Ñ€ÐµÐ°Ð»Ð¸Ð·Ð¾Ð²Ð°Ñ‚ÑŒ Ð¼ÐµÑ…Ð°Ð½Ð¸Ð·Ð¼ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ \"ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½Ñ‹Ñ… Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¹\" (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, Ñ‡Ñ‚Ð¾-Ñ‚Ð¾ Ð²Ñ€Ð¾Ð´Ðµ Ñ‚Ð°ÐºÐ¾Ð³Ð¾: $f(x) = x + 5 * x - \\cos(20 * \\log(12 - 20 * x * x )) - 20 * x$ )\n",
    "- ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ Ð±Ð¾Ð»ÑŒÑˆÐ¾Ðµ Ñ‡Ð¸ÑÐ»Ð¾ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¹ Ð¸ ÑÑ€Ð°Ð²Ð½Ð¸Ñ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ ÑÐ¸Ð¼Ð²Ð¾Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¸ Ñ‡Ð¸ÑÐ»ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð´Ð¸Ñ„Ñ„ÐµÑ€ÐµÐ½Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð² ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½Ñ‹Ñ… Ñ‚Ð¾Ñ‡ÐºÐ°Ñ… "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸ÑŽ ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½Ñ‹Ñ… Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¹ Ð¼Ð¾Ð¶Ð½Ð¾ Ð¾ÑÑƒÑ‰ÐµÑÑ‚Ð²Ð¸Ñ‚ÑŒ, Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, Ð´Ð²ÑƒÐ¼Ñ Ð¿ÑƒÑ‚ÑÐ¼Ð¸. \n",
    "1. Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ñ„ÑƒÐ½ÐºÑ†Ð¸ÑŽ Ð² Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ð¾Ð¼ Ð²Ð¸Ð´Ðµ, Ð·Ð°Ñ‡ÐµÐ¼ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð²ÑÑ‚Ñ€Ð¾ÐµÐ½Ð½ÑƒÑŽ Ñ„ÑƒÐ½ÐºÑ†Ð¸ÑŽ [eval](https://docs.python.org/3/library/functions.html#eval)\n",
    "\n",
    "```python\n",
    "func = eval(\"lambda x: 2 * x + 5\")\n",
    "assert func(42) == 89 \n",
    "```\n",
    "\n",
    "2. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ñ‹Ð¹ Ð¼Ð¾Ð´ÑƒÐ»ÑŒ [ast](https://docs.python.org/3/library/ast.html), ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð²Ð¾ Ð²Ñ€ÐµÐ¼Ñ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñ‹ Ð¼Ð°Ð½Ð¸Ð¿ÑƒÐ»Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ [ÐÐ±ÑÑ‚Ñ€Ð°ÐºÑ‚Ð½Ñ‹Ð¼ Ð¡Ð¸Ð½Ñ‚Ð°ÐºÑÐ¸Ñ‡ÐµÑÐºÐ¸Ð¼ Ð”ÐµÑ€ÐµÐ²Ð¾Ð¼](https://ru.wikipedia.org/wiki/%D0%90%D0%B1%D1%81%D1%82%D1%80%D0%B0%D0%BA%D1%82%D0%BD%D0%BE%D0%B5_%D1%81%D0%B8%D0%BD%D1%82%D0%B0%D0%BA%D1%81%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B5_%D0%B4%D0%B5%D1%80%D0%B5%D0%B2%D0%BE).\n",
    "ÐÐ°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, Ð²Ñ‹Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ \n",
    "\n",
    "```python\n",
    "func = lambda x: 2 * x + 5\n",
    "```\n",
    "\n",
    "ÐœÐ¾Ð¶Ð½Ð¾ Ð·Ð°Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ ÐºÐ¾Ð´Ð°:\n",
    "\n",
    "```python\n",
    "\n",
    "expr = ast.Expression(\n",
    "    body=ast.Lambda(\n",
    "        args=ast.arguments(\n",
    "            args=[\n",
    "                ast.arg(arg='x')\n",
    "            ],\n",
    "            posonlyargs=[],\n",
    "            kwonlyargs=[],\n",
    "            kw_defaults=[],\n",
    "            defaults=[]\n",
    "        ),\n",
    "        body=ast.BinOp(\n",
    "            left=ast.BinOp(\n",
    "                left=ast.Constant(value=2),\n",
    "                op=ast.Mult(),\n",
    "                right=ast.Name(id='x', ctx=ast.Load())\n",
    "            ),\n",
    "            op=ast.Add(),\n",
    "            right=ast.Constant(value=5)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "ast.fix_missing_locations(expr)\n",
    "\n",
    "func = eval(compile(expr, filename=\"\", mode=\"eval\"))\n",
    "\n",
    "assert func(42) == 89\n",
    "```\n",
    "\n",
    "ÐŸÑ€Ð¸ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð½ÑƒÐ¶Ð½Ð¾ ÑƒÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°Ñ‚ÑŒ Ð¾Ð±Ð»Ð°ÑÑ‚Ð¸ Ð´Ð¾Ð¿ÑƒÑÑ‚Ð¸Ð¼Ñ‹Ñ… Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¹."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ð—Ð°Ð´Ð°Ð½Ð¸Ðµ 1.5 (7 Ð±Ð°Ð»Ð»Ð¾Ð²)\n",
    "\n",
    "Ð ÐµÐ°Ð»Ð¸Ð·ÑƒÐ¹Ñ‚Ðµ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÑƒ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¹ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð². ÐÐ°Ð¿Ñ€Ð¸Ð¼ÐµÑ€\n",
    "\n",
    "```python\n",
    "def f(x: float, y: float, z: float) -> float:\n",
    "    return x * y + z - 5 * y  \n",
    "\n",
    "\n",
    "f_diff = diff(f)\n",
    "\n",
    "f_diff(10, 10, 10) # = [10, 5, 1]\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's take a look at the task as a computational graph (what it actually is) ~~say 'hello' to TensorFlow~~. Code beneath covers tasks 1.1, 1.2 and 1.5"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "source": [
    "import numpy as np\n",
    "import sympy\n",
    "import random\n",
    "from abc import ABC, abstractmethod"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "source": [
    "np.random.seed(2023)\n",
    "random.seed(2023)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "source": [
    "class Node(ABC):\n",
    "    \"\"\"Node of computational graph. Abstract class.\"\"\"    \n",
    "    @abstractmethod\n",
    "    def backward(self, var):\n",
    "        \"\"\"Create new node in graph.\n",
    "        \"\"\"        \n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def compute(self):\n",
    "        \"\"\"TBD.\"\"\"        \n",
    "        pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "source": [
    "class Const(Node):\n",
    "    \"\"\"Representates const type of node.\n",
    "    \"\"\"    \n",
    "    def __init__(self, value: float | int):\n",
    "        self.value = value\n",
    "    \n",
    "    def backward(self, var):\n",
    "        return Const(0.0)\n",
    "\n",
    "    def compute(self):\n",
    "        return self.value\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.value)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "source": [
    "class Variable(Node):\n",
    "    \"\"\"Representation of variable.\n",
    "    \"\"\"    \n",
    "    def __init__(self, name, value=None):\n",
    "        self.name = name\n",
    "        self.value = value\n",
    "    \n",
    "    def backward(self, var):\n",
    "        return Const(1) if self == var else Const(0)\n",
    "\n",
    "    def compute(self):\n",
    "        if self.value is None:\n",
    "            raise ValueError('variable seems to be empty')\n",
    "        return self.value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.name}'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "source": [
    "class Sum(Node):\n",
    "    \"\"\"Summary operation.\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y):\n",
    "        self.x, self.y = x, y\n",
    "    \n",
    "    def backward(self, var):\n",
    "        return Sum(self.x.backward(var), self.y.backward(var))\n",
    "\n",
    "    def compute(self):\n",
    "        return self.x.compute() + self.y.compute()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'({self.x} + {self.y})'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "source": [
    "class Mul(Node):\n",
    "    \"\"\"Multiply operation.\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y):\n",
    "        self.x, self.y = x, y\n",
    "    \n",
    "    def backward(self, var):\n",
    "        return Sum(\n",
    "            Mul(self.x.backward(var), self.y),\n",
    "            Mul(self.x, self.y.backward(var))\n",
    "        )\n",
    "\n",
    "    def compute(self):\n",
    "        return self.x.compute() * self.y.compute()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'({self.x} * {self.y})'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "source": [
    "class Neg(Node):\n",
    "    \"\"\"Change sign of Node to opposite.\n",
    "    x > 0 -> Neg(x) -> x < 0\n",
    "    \"\"\"\n",
    "    def __init__(self, x):\n",
    "        self.x =  x\n",
    "    \n",
    "    def backward(self, var):\n",
    "        return Neg(self.x.backward(var))\n",
    "    \n",
    "    def compute(self):\n",
    "        return - self.x.compute()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'(-{self.x})'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "source": [
    "class Power(Node):\n",
    "    \"\"\"Power operation.\n",
    "    Notice, that first argument raises to power from second argument.\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y):\n",
    "        self.x, self.y = x, y\n",
    "    \n",
    "    def backward(self, var):\n",
    "        return Sum(\n",
    "            Mul(Mul(self.y, Power(self.x, Sum(self.y, Const(-1)))), self.x.backward(var)),\n",
    "            Mul(Mul(Power(self.x, self.y), self.y.backward(var)), Log(self.x))\n",
    "        )\n",
    "    \n",
    "    def compute(self):\n",
    "        return self.x.compute() ** self.y.compute()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'({self.x} ** {self.y})'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "source": [
    "class Divide(Node):\n",
    "    \"\"\"Divide first argument to the second.\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y):\n",
    "        self.x, self.y = x, y\n",
    "    \n",
    "    def backward(self, var):\n",
    "        return Divide(\n",
    "            Sum(\n",
    "                Mul(self.x.backward(var), self.y),\n",
    "                Neg(Mul(self.y.backward(var), self.x))\n",
    "            ),\n",
    "            Power(self.y, Const(2))\n",
    "        )\n",
    "    \n",
    "    def compute(self):\n",
    "        return self.x.compute() / self.y.compute()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'({self.x} / {self.y})'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "source": [
    "class Abs(Node):\n",
    "    \"\"\"Represent absolute operation. \n",
    "    Notice that abs(x) -- not smooth-function, so it may have some unexpected side-effects ðŸ¤“.\\n\n",
    "    This implementaion is based on assumption that if submodule expression is lower than zero,\n",
    "    then it's diff and value change their signs. Otherwise nothing happens.\n",
    "    \"\"\"\n",
    "    def __init__(self, x):\n",
    "        self.x =  x\n",
    "    \n",
    "    def backward(self, var):\n",
    "        return Abs(self.x.backward(var))\n",
    "    \n",
    "    def compute(self):\n",
    "        value = self.x.compute()\n",
    "        if value == 0:\n",
    "            return 0\n",
    "        return value if value > 0 else -value\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'(|{self.x}|)'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "source": [
    "x = Variable('x', 3)\n",
    "y = Variable('y', 2)\n",
    "\n",
    "z = Abs(Sum(y, Neg(x)))\n",
    "\n",
    "z\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(|(y + (-x))|)"
      ]
     },
     "metadata": {},
     "execution_count": 347
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "source": [
    "z.backward(x)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(|(0 + (-1))|)"
      ]
     },
     "metadata": {},
     "execution_count": 348
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "source": [
    "z.backward(x).compute()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 349
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "source": [
    "#x * y + z - 5 * y  \n",
    "x = Variable('x', 10)\n",
    "y = Variable('y', 10)\n",
    "z = Variable('z', 10)\n",
    "\n",
    "f = Sum(\n",
    "    Sum(\n",
    "        Mul(x,y),\n",
    "        z \n",
    "    ), Neg(Mul(Const(5),y))\n",
    ")\n",
    "f"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(((x * y) + z) + (-(5 * y)))"
      ]
     },
     "metadata": {},
     "execution_count": 350
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "source": [
    "f.backward(x).compute()\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "metadata": {},
     "execution_count": 351
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "source": [
    "x = Variable('x', 3)\n",
    "y = Variable('y', 2)\n",
    "z = Power(x,y)\n",
    "z"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(x ** y)"
      ]
     },
     "metadata": {},
     "execution_count": 352
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "source": [
    "z.compute()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "metadata": {},
     "execution_count": 353
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "source": [
    "z.backward(y)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(((y * (x ** (y + -1))) * 0) + (((x ** y) * 1) * log(x)))"
      ]
     },
     "metadata": {},
     "execution_count": 354
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "source": [
    "z.backward(y).compute()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9.887510598012987"
      ]
     },
     "metadata": {},
     "execution_count": 355
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's make our toy auto diff easier to use: add symbolic operations. To not-built-in functions add our mixin to class directly:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "source": [
    "from typing import Any\n",
    "class AutoGradMixin:\n",
    "    \"\"\"Mixin for covering all nodes with symbolic operators.\n",
    "    \"\"\"    \n",
    "    @staticmethod\n",
    "    def _to_node(x: Node | Any):\n",
    "        \"\"\"check if 'x' is Node, otherwise create Const object base on 'x'\n",
    "\n",
    "        Args:\n",
    "            x (Node | Any): Node or basement for a new Const\n",
    "        \"\"\"\n",
    "        return x if isinstance(x, Node) else Const(x) \n",
    "\n",
    "    def __add__(self, other):\n",
    "        return SymbolicSum(self, self._to_node(other))\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return SymbolicSum(self._to_node(other), self)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return SymbolicMul(self, self._to_node(other))     \n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return SymbolicMul(self._to_node(other), self)   \n",
    "\n",
    "    def __neg__(self):\n",
    "        return SymbolicNeg(self)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return SymbolicSum(self, SymbolicNeg(self._to_node(other)))\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        return SymbolicSum(self._to_node(other), SymbolicNeg(self))\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        return SymbolicPower(self, self._to_node(other))\n",
    "    \n",
    "    def __rpow__(self, other):\n",
    "        return SymbolicPower(self._to_node(other), self)\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return SymbolicDivide(self, self._to_node(other))\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        return SymbolicDivide(self._to_node(other), self)\n",
    "    \n",
    "    def __abs__(self):\n",
    "        return Abs(self)\n",
    "\n",
    "class SymbolicVar(Variable, AutoGradMixin):\n",
    "    pass\n",
    "\n",
    "class SymbolicConst(Const, AutoGradMixin):\n",
    "    pass\n",
    "\n",
    "class SymbolicSum(Sum, AutoGradMixin):\n",
    "    pass\n",
    "\n",
    "class SymbolicMul(Mul, AutoGradMixin):\n",
    "    pass\n",
    "\n",
    "class SymbolicNeg(Neg, AutoGradMixin):\n",
    "    pass\n",
    "\n",
    "class SymbolicPower(Power, AutoGradMixin):\n",
    "    pass\n",
    "\n",
    "class SymbolicDivide(Divide, AutoGradMixin):\n",
    "    pass\n",
    "\n",
    "class Abs(Node, AutoGradMixin):\n",
    "    \"\"\"Represent absolute operation. \n",
    "    Notice that abs(x) -- not smooth-function, so it may have some unexpected side-effects ðŸ¤“.\\n\n",
    "    This implementaion is based on assumption that if submodule expression is lower than zero,\n",
    "    then it's diff and value change their signs. Otherwise nothing happens.\n",
    "    \"\"\"\n",
    "    def __init__(self, x):\n",
    "        self.x =  x\n",
    "    \n",
    "    def backward(self, var):\n",
    "        return Abs(self.x.backward(var))\n",
    "    \n",
    "    def compute(self):\n",
    "        value = self.x.compute()\n",
    "        if value == 0:\n",
    "            return 0\n",
    "        return value if value > 0 else -value\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'(|{self.x}|)'\n",
    "\n",
    "class Log(Node, AutoGradMixin):\n",
    "    \"\"\"Represent natural logarithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, x) -> None:\n",
    "        self.x = x\n",
    "    \n",
    "    def backward(self, var):\n",
    "        return Mul(\n",
    "            Divide(Const(1), self.x),\n",
    "            self.x.backward(var)\n",
    "            )\n",
    "    \n",
    "    def compute(self):\n",
    "        return np.log(self.x.compute())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'log({self.x})'\n",
    "\n",
    "class Exp(Node, AutoGradMixin):\n",
    "    \"\"\"Represent exponential operation\n",
    "    \"\"\"\n",
    "    def __init__(self, x) -> None:\n",
    "        self.x = x\n",
    "    \n",
    "    def backward(self, var):\n",
    "        return Mul(Exp(self.x), self.x.backward(var))\n",
    "    \n",
    "    def compute(self):\n",
    "        return np.exp(self.x.compute())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'e^({self.x})'\n",
    "\n",
    "class Cos(Node, AutoGradMixin):\n",
    "    \"\"\"Represent cosine operation\n",
    "    \"\"\"\n",
    "    def __init__(self, x) -> None:\n",
    "        self.x = x\n",
    "    \n",
    "    def backward(self, var):\n",
    "        return Mul(Neg(Sin(self.x)), self.x.backward(var))\n",
    "    \n",
    "    def compute(self):\n",
    "        return np.cos(self.x.compute())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'cos({self.x})'\n",
    "\n",
    "class Sin(Node, AutoGradMixin):\n",
    "    \"\"\"Represent sinus operation\n",
    "    \"\"\"\n",
    "    def __init__(self, x) -> None:\n",
    "        self.x = x\n",
    "    \n",
    "    def backward(self, var):\n",
    "        return Mul(Cos(self.x), self.x.backward(var))\n",
    "    \n",
    "    def compute(self):\n",
    "        return np.sin(self.x.compute())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'sin({self.x})'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing time! ðŸ› \n",
    "\n",
    "Let's use just simple \"asserts\" with sympy for this moment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "source": [
    "#testing binary ops, division and powering\n",
    "x_value, y_value = np.random.randint(1,100), np.random.randint(1,10)\n",
    "x = SymbolicVar('x', x_value)\n",
    "y = SymbolicVar('y', y_value)\n",
    "z1 = x * x + x * y * 5 + 4\n",
    "z1_diff_x = z1.backward(x).compute()\n",
    "z1_diff_y = z1.backward(y).compute()\n",
    "\n",
    "z2 = x ** y\n",
    "z2_diff_x = z2.backward(x).compute()\n",
    "z2_diff_y = z2.backward(y).compute()\n",
    "\n",
    "z3 = (3 * x) / y\n",
    "z3_diff_x = z3.backward(x).compute()\n",
    "z3_diff_y = z3.backward(y).compute()\n",
    "\n",
    "#sympy setup\n",
    "x = sympy.Symbol('x')\n",
    "y = sympy.Symbol('y')\n",
    "z1 = x * x + x * y * 5 + 4\n",
    "assert sympy.lambdify([x,y], sympy.diff(z1, x))(x_value,y_value) == z1_diff_x\n",
    "assert sympy.lambdify([x,y], sympy.diff(z1, y))(x_value,y_value) == z1_diff_y\n",
    "\n",
    "z2 = x ** y\n",
    "assert sympy.lambdify([x,y], sympy.diff(z2, x))(x_value,y_value) == z2_diff_x\n",
    "assert sympy.lambdify([x,y], sympy.diff(z2, y))(x_value,y_value) == z2_diff_y\n",
    "\n",
    "z3 = (3 * x) / y\n",
    "assert sympy.lambdify([x,y], sympy.diff(z3, x))(x_value,y_value) == z3_diff_x\n",
    "assert sympy.lambdify([x,y], sympy.diff(z3, y))(x_value,y_value) == z3_diff_y\n",
    "print(\"Passed  ðŸŽ‰\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Passed  ðŸŽ‰\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "source": [
    "#testing exp, cos, sin, log\n",
    "x_value, y_value = np.random.randint(1,100), np.random.randint(1,10)\n",
    "x = SymbolicVar('x', x_value)\n",
    "y = SymbolicVar('y', y_value)\n",
    "z1 = Exp(x + y)\n",
    "z1_diff_x = z1.backward(x).compute()\n",
    "z1_diff_y = z1.backward(y).compute()\n",
    "\n",
    "z2 = Cos(x - y)\n",
    "z2_diff_x = z2.backward(x).compute()\n",
    "z2_diff_y = z2.backward(y).compute()\n",
    "\n",
    "z3 = Sin(x * y)\n",
    "z3_diff_x = z3.backward(x).compute()\n",
    "z3_diff_y = z3.backward(y).compute()\n",
    "\n",
    "z4 = Log(Sin(x) + Cos(y))\n",
    "z4_diff_x = z4.backward(x).compute()\n",
    "z4_diff_y = z4.backward(y).compute()\n",
    "\n",
    "#sympy setup\n",
    "x = sympy.Symbol('x')\n",
    "y = sympy.Symbol('y')\n",
    "z1 = sympy.exp(x + y)\n",
    "assert sympy.lambdify([x,y], sympy.diff(z1, x))(x_value,y_value) == z1_diff_x\n",
    "assert sympy.lambdify([x,y], sympy.diff(z1, y))(x_value,y_value) == z1_diff_y\n",
    "\n",
    "z2 = sympy.cos(x - y)\n",
    "assert sympy.lambdify([x,y], sympy.diff(z2, x))(x_value,y_value) == z2_diff_x\n",
    "assert sympy.lambdify([x,y], sympy.diff(z2, y))(x_value,y_value) == z2_diff_y\n",
    "\n",
    "z3 = sympy.sin(x * y)\n",
    "assert sympy.lambdify([x,y], sympy.diff(z3, x))(x_value,y_value) == z3_diff_x\n",
    "assert sympy.lambdify([x,y], sympy.diff(z3, y))(x_value,y_value) == z3_diff_y\n",
    "\n",
    "z4 = sympy.log(sympy.sin(x) + sympy.cos(y))\n",
    "assert sympy.lambdify([x,y], sympy.diff(z4, x))(x_value,y_value) == z4_diff_x\n",
    "assert sympy.lambdify([x,y], sympy.diff(z4, y))(x_value,y_value) == z4_diff_y\n",
    "print(\"Passed  ðŸŽ‰\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Passed  ðŸŽ‰\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Almost here ðŸŽ‰ Two steps left: implement own numerical differentation mechanism & generator of testing functions. Let's start with first one (covers 1.3 task).\n",
    "\n",
    "**Newton's difference quotient** -- according to [wiki](https://en.wikipedia.org/wiki/Numerical_differentiation), our diff can be represented as finite difference approximations.\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "source": [
    "def partial_difference_quotient(f, var, i, step):\n",
    "    \"\"\"compute the i-th partial difference quotient of function f at var\"\"\"\n",
    "    var_step = [var_j + (step if j == i else 0) for j, var_j in enumerate(var)]\n",
    "\n",
    "    return (f(var_step) - f(var)) / step #slope\n",
    "\n",
    "def estimate_gradient(f, var, step=0.00001):\n",
    "    return [partial_difference_quotient(f, var, i, step) for i, _ in enumerate(var)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "source": [
    "x_value, y_value = np.random.randint(1,100), np.random.randint(1,10)\n",
    "print(f\"x: {x_value}, y: {y_value}\")\n",
    "x = SymbolicVar('x', x_value)\n",
    "y = SymbolicVar('y', y_value)\n",
    "z = x * x + x * y * 3 + 1 \n",
    "print(f\"autograd dz/dx = {z.backward(x).compute()}\")\n",
    "print(f\"autograd dz/dy = {z.backward(y).compute()}\")\n",
    "\n",
    "def f(v):\n",
    "    x, y = v\n",
    "    return x * x + x * y * 3 + 1 \n",
    "\n",
    "var = [x_value, y_value]\n",
    "grad = estimate_gradient(f, var, step = 0.00001)\n",
    "formatted_grad = [ '%.2f' % elem for elem in grad ]\n",
    "print(f\"numerical diff dz/dx ~= {formatted_grad[0]} (full form: {grad[0]})\")\n",
    "print(f\"numerical diff dz/dy ~= {formatted_grad[1]} (full form: {grad[1]})\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x: 93, y: 4\n",
      "autograd dz/dx = 198.0\n",
      "autograd dz/dy = 279.0\n",
      "numerical diff dz/dx ~= 198.00 (full form: 198.0000100957113)\n",
      "numerical diff dz/dy ~= 279.00 (full form: 279.00000004592584)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "source": [
    "x_value, y_value = np.random.randint(1,100), np.random.randint(1,10)\n",
    "print(f\"x: {x_value}, y: {y_value}\")\n",
    "x = SymbolicVar('x', x_value)\n",
    "y = SymbolicVar('y', y_value)\n",
    "z = Abs(x-y)\n",
    "print(f\"autograd dz/dx = {z.backward(x).compute()}\")\n",
    "print(f\"autograd dz/dy = {z.backward(y).compute()}\")\n",
    "\n",
    "def f(v):\n",
    "    x, y = v\n",
    "    return abs(x - y)\n",
    "\n",
    "var = [x_value, y_value]\n",
    "grad = estimate_gradient(f, var, step = 0.00001)\n",
    "formatted_grad = [ '%.2f' % elem for elem in grad ]\n",
    "print(f\"numerical diff dz/dx ~= {formatted_grad[0]} (full form: {grad[0]})\")\n",
    "print(f\"numerical diff dz/dy ~= {formatted_grad[1]} (full form: {grad[1]})\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x: 53, y: 7\n",
      "autograd dz/dx = 1\n",
      "autograd dz/dy = 1\n",
      "numerical diff dz/dx ~= 1.00 (full form: 1.0000000003174137)\n",
      "numerical diff dz/dy ~= -1.00 (full form: -1.0000000003174137)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, pretty close to our auto-diff's results! And last but now least step: let's create toy functions' generator.\n",
    "\n",
    "It would be nice to test if result of auto-diff of generated function is the same as result of numerical diff of generated function, so we will keep this idea in mind and write method which generates mathematically the same function but in two different forms - as an input for auto-diff and as an input for numerical one.\n",
    "\n",
    "Additionally, let's fully cover task 1.5 by allow to generate multivariable functions\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "source": [
    "import ast\n",
    "import math\n",
    "from typing import Callable"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "source": [
    "def _arg_name(i: int):\n",
    "    return f\"x{i}\"\n",
    "\n",
    "def _random_expr(args_amount: int, recursion_limit: int):\n",
    "    recursion_limit -= 1\n",
    "\n",
    "    if recursion_limit == 1:\n",
    "        unary_node_without_recursive_calls_type = random.choice([\n",
    "            'const',\n",
    "            'var',\n",
    "        ])\n",
    "        if unary_node_without_recursive_calls_type == 'const':\n",
    "            value = random.randint(-10, 10)\n",
    "            return [\n",
    "                ast.Call(ast.Name(id='SymbolicConst', ctx=ast.Load()), args=[ast.Constant(value)], keywords=[]),\n",
    "                ast.Constant(value)\n",
    "            ] \n",
    "        elif unary_node_without_recursive_calls_type == 'var':\n",
    "            name = _arg_name(random.randint(0,args_amount-1))\n",
    "            return [ast.Name(id=name, ctx=ast.Load()), ast.Name(id=name, ctx=ast.Load())]\n",
    "\n",
    "\n",
    "    # choose binary or unary node\n",
    "    if random.random() < 0.4:\n",
    "        # expression will be bin op\n",
    "        left = _random_expr(args_amount,recursion_limit)\n",
    "        right = _random_expr(args_amount, recursion_limit)\n",
    "        op = random.choice([ast.Add(), ast.Sub(), ast.Mult(), ast.Div(), ast.Pow()])\n",
    "        return [\n",
    "            ast.BinOp(\n",
    "                left=left[0],\n",
    "                op=op,\n",
    "                right=right[0]\n",
    "            ),\n",
    "            ast.BinOp(\n",
    "                left=left[1],\n",
    "                op=op,\n",
    "                right=right[1]\n",
    "            )\n",
    "        ]\n",
    "    else:\n",
    "        # expression will be single node\n",
    "        unary_node_type = random.choice([\n",
    "            'const',\n",
    "            'var',\n",
    "            'abs',\n",
    "            'cos',\n",
    "            'sin',\n",
    "            'exp',\n",
    "            'log',\n",
    "        ])\n",
    "        if unary_node_type == 'const':\n",
    "            value = random.randint(-10, 10)\n",
    "            return [\n",
    "                ast.Call(ast.Name(id='SymbolicConst', ctx=ast.Load()), args=[ast.Constant(value)], keywords=[]),\n",
    "                ast.Constant(value)\n",
    "            ]\n",
    "        if unary_node_type == 'var':\n",
    "            name = _arg_name(random.randint(0,args_amount-1))\n",
    "            return [ast.Name(id=name, ctx=ast.Load()), ast.Name(id=name, ctx=ast.Load())]\n",
    "        elif unary_node_type == 'abs':\n",
    "            expr = _random_expr(args_amount, recursion_limit)\n",
    "            return [\n",
    "                ast.Call(ast.Name(id='Abs', ctx=ast.Load()), args=[expr[0]], keywords=[]),\n",
    "                ast.Call(ast.Name(id='abs', ctx=ast.Load()), args=[expr[1]], keywords=[])\n",
    "            ]\n",
    "        elif unary_node_type == 'cos':\n",
    "            expr = _random_expr(args_amount, recursion_limit)\n",
    "            return [\n",
    "                ast.Call(ast.Name(id='Cos', ctx=ast.Load()), args=[expr[0]], keywords=[]),\n",
    "                ast.Call(ast.Name(id='cos', ctx=ast.Load()), args=[expr[1]], keywords=[])\n",
    "            ]\n",
    "        elif unary_node_type == 'sin':\n",
    "            expr = _random_expr(args_amount, recursion_limit)\n",
    "            return [\n",
    "                ast.Call(ast.Name(id='Sin', ctx=ast.Load()), args=[expr[0]], keywords=[]),\n",
    "                ast.Call(ast.Name(id='sin', ctx=ast.Load()), args=[expr[1]], keywords=[])\n",
    "            ]\n",
    "        elif unary_node_type == 'log':\n",
    "            expr = _random_expr(args_amount, recursion_limit)\n",
    "            return [\n",
    "                ast.Call(ast.Name(id='Log', ctx=ast.Load()), args=[expr[0]], keywords=[]),\n",
    "                ast.Call(ast.Name(id='log', ctx=ast.Load()), args=[expr[1]], keywords=[])\n",
    "            ]\n",
    "        elif unary_node_type == 'exp':\n",
    "            expr = _random_expr(args_amount, recursion_limit)\n",
    "            return [\n",
    "                ast.Call(ast.Name(id='Exp', ctx=ast.Load()), args=[expr[0]], keywords=[]),\n",
    "                ast.Call(ast.Name(id='exp', ctx=ast.Load()), args=[expr[1]], keywords=[])\n",
    "            ]\n",
    "\n",
    "\n",
    "def generate_function(args_amount: int = 1, recursion_limit: int = 10, verbose=True) -> Callable[[float], float]:\n",
    "    body = _random_expr(args_amount, recursion_limit)\n",
    "    expr = ast.Expression(\n",
    "        body=ast.Lambda(\n",
    "            args=ast.arguments(\n",
    "                args=[\n",
    "                    ast.arg(arg=_arg_name(i))\n",
    "                    for i in range(args_amount)\n",
    "                ],\n",
    "                posonlyargs=[],\n",
    "                kwonlyargs=[],\n",
    "                kw_defaults=[],\n",
    "                defaults=[]\n",
    "            ),\n",
    "            body=body[0]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    expr_for_num_diff = ast.Expression(\n",
    "        body=ast.Lambda(\n",
    "            args=ast.arguments(\n",
    "                args=[\n",
    "                    ast.arg(arg=_arg_name(i))\n",
    "                    for i in range(args_amount)\n",
    "                ],\n",
    "                posonlyargs=[],\n",
    "                kwonlyargs=[],\n",
    "                kw_defaults=[],\n",
    "                defaults=[]\n",
    "            ),\n",
    "            body=body[1]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    ast.fix_missing_locations(expr)\n",
    "    ast.fix_missing_locations(expr_for_num_diff)\n",
    "    if verbose:\n",
    "        print(f\"Generated function for auto-diff: {ast.unparse(expr)}\")\n",
    "        print(f\"Generated function for numerical diff: {ast.unparse(expr_for_num_diff)}\\n\")\n",
    "    compiled_func = compile(expr, filename=\"\", mode=\"eval\")\n",
    "    compiled_func_for_num_diff = compile(expr_for_num_diff, filename=\"\", mode=\"eval\")\n",
    "    func = eval(compiled_func, {'Abs': Abs, 'Cos': Cos, 'Sin': Sin, 'Log': Log, 'Exp': Exp, 'SymbolicConst': SymbolicConst})\n",
    "    func_for_numerical_diff = eval(compiled_func_for_num_diff, {'abs':abs, 'cos': math.cos, 'sin': math.sin, 'log': math.log,'exp': math.exp})\n",
    "    return func, func_for_numerical_diff"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our toy auto-gen sometimes produces funny things, such as many stacked exponents which causes inf as a result; unfortunately, because we generate functions randomly, this kind of behaviour is necessary evil, just reload cell ðŸ¤“ (~~probably it can be fixed somehow at least author doesn't know how~~)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "But let's take a look at the result of all things below:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "source": [
    "x = SymbolicVar('x', 10)\n",
    "func, func4num = generate_function(recursion_limit=4)\n",
    "print(f\"Generated function: {func(x)}\")\n",
    "print(f\"Generated function at point {x.value}: {func(x).compute():.2f}\")\n",
    "print(f\"Generated function derivative: {func(x).backward(x)}\")\n",
    "print(f\"Generated function derivative at point {x.value}: {func(x).backward(x).compute():.2f}\\n\")\n",
    "\n",
    "def f(v):\n",
    "    x = v[0]\n",
    "    return func4num(x)\n",
    "\n",
    "var = [x.value]\n",
    "grad = estimate_gradient(f, var, step = 0.00001)\n",
    "print(f\"Generated function numerical derivative at point {x.value}: {grad[0]:.2f}\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generated function for auto-diff: lambda x0: Cos(SymbolicConst(-7) + x0)\n",
      "Generated function for numerical diff: lambda x0: cos(-7 + x0)\n",
      "\n",
      "Generated function: cos((-7 + x))\n",
      "Generated function at point 10: -0.99\n",
      "Generated function derivative: ((-sin((-7 + x))) * (0.0 + 1))\n",
      "Generated function derivative at point 10: -0.14\n",
      "\n",
      "Generated function numerical derivative at point 10: -0.14\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "source": [
    "x = SymbolicVar('x', 10)\n",
    "y = SymbolicVar('y', 5)\n",
    "func, func4num = generate_function(args_amount=2, recursion_limit=5)\n",
    "print(f\"Generated function: {func(x, y)}\")\n",
    "print(f\"Generated function at point ({x.value}, {y.value}): {func(x, y).compute():.2f}\")\n",
    "print(f\"Generated function derivative by x: {func(x,y).backward(x)}\")\n",
    "print(f\"Generated function derivative by y: {func(x,y).backward(y)}\")\n",
    "print(f\"Generated function derivative by x at point ({x.value}, {y.value}): {func(x,y).backward(x).compute():.2f}\")\n",
    "print(f\"Generated function derivative by y at point ({x.value}, {y.value}): {func(x,y).backward(y).compute():.2f}\\n\")\n",
    "\n",
    "def f(v):\n",
    "    x, y = v\n",
    "    return func4num(x, y)\n",
    "\n",
    "var = [x.value, y.value]\n",
    "grad = estimate_gradient(f, var, step = 0.00001)\n",
    "formatted_grad = [ '%.2f' % elem for elem in grad ]\n",
    "print(f\"numerical diff dz/dx ~= {formatted_grad[0]} (full form: {grad[0]})\")\n",
    "print(f\"numerical diff dz/dy ~= {formatted_grad[1]} (full form: {grad[1]})\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generated function for auto-diff: lambda x0, x1: Abs(Cos(x1))\n",
      "Generated function for numerical diff: lambda x0, x1: abs(cos(x1))\n",
      "\n",
      "Generated function: (|cos(y)|)\n",
      "Generated function at point (10, 5): 0.28\n",
      "Generated function derivative by x: (|((-sin(y)) * 0)|)\n",
      "Generated function derivative by y: (|((-sin(y)) * 1)|)\n",
      "Generated function derivative by x at point (10, 5): 0.00\n",
      "Generated function derivative by y at point (10, 5): 0.96\n",
      "\n",
      "numerical diff dz/dx ~= 0.00 (full form: 0.0)\n",
      "numerical diff dz/dy ~= 0.96 (full form: 0.9589228563033901)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "source": [
    "x1 = SymbolicVar('x1', 10)\n",
    "x2 = SymbolicVar('x2', 5)\n",
    "x3 = SymbolicVar('x3', -2)\n",
    "func, func4num = generate_function(args_amount=3, recursion_limit=5)\n",
    "print(f\"Generated function: {func(x1, x2, x3)}\")\n",
    "print(f\"Generated function at point ({x1.value}, {x2.value}, {x3.value}): {func(x1, x2, x3).compute():.2f}\\n\")\n",
    "print(f\"Generated function derivative by x1: {func(x1, x2, x3).backward(x1)}\")\n",
    "print(f\"Generated function derivative by x2: {func(x1, x2, x3).backward(x2)}\")\n",
    "print(f\"Generated function derivative by x3: {func(x1, x2, x3).backward(x3)}\\n\")\n",
    "print(f\"Generated function derivative by x1 at point ({x1.value}, {x2.value}, {x3.value}): {func(x1, x2, x3).backward(x1).compute():.2f}\")\n",
    "print(f\"Generated function derivative by x2 at point ({x1.value}, {x2.value}, {x3.value}): {func(x1, x2, x3).backward(x2).compute():.2f}\")\n",
    "print(f\"Generated function derivative by x3 at point ({x1.value}, {x2.value}, {x3.value}): {func(x1, x2, x3).backward(x3).compute():.2f}\\n\")\n",
    "\n",
    "def f(v):\n",
    "    x1, x2, x3 = v\n",
    "    return func4num(x1, x2, x3)\n",
    "\n",
    "var = [x1.value, x2.value, x3.value]\n",
    "grad = estimate_gradient(f, var, step = 0.00001)\n",
    "formatted_grad = [ '%.2f' % elem for elem in grad ]\n",
    "print(f\"numerical diff dz/dx1 ~= {formatted_grad[0]} (full form: {grad[0]})\")\n",
    "print(f\"numerical diff dz/dx2 ~= {formatted_grad[1]} (full form: {grad[1]})\")\n",
    "print(f\"numerical diff dz/dx3 ~= {formatted_grad[2]} (full form: {grad[2]})\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generated function for auto-diff: lambda x0, x1, x2: (x2 / x0 - Log(x1)) * Sin(x1 - SymbolicConst(-1))\n",
      "Generated function for numerical diff: lambda x0, x1, x2: (x2 / x0 - log(x1)) * sin(x1 - -1)\n",
      "\n",
      "Generated function: (((x3 / x1) + (-log(x2))) * sin((x2 + (--1))))\n",
      "Generated function at point (10, 5, -2): 0.51\n",
      "\n",
      "Generated function derivative by x1: ((((((0 * x1) + (-(1 * x3))) / (x1 ** 2)) + (-((1 / x2) * 0))) * sin((x2 + (--1)))) + (((x3 / x1) + (-log(x2))) * (cos((x2 + (--1))) * (0 + (-0.0)))))\n",
      "Generated function derivative by x2: ((((((0 * x1) + (-(0 * x3))) / (x1 ** 2)) + (-((1 / x2) * 1))) * sin((x2 + (--1)))) + (((x3 / x1) + (-log(x2))) * (cos((x2 + (--1))) * (1 + (-0.0)))))\n",
      "Generated function derivative by x3: ((((((1 * x1) + (-(0 * x3))) / (x1 ** 2)) + (-((1 / x2) * 0))) * sin((x2 + (--1)))) + (((x3 / x1) + (-log(x2))) * (cos((x2 + (--1))) * (0 + (-0.0)))))\n",
      "\n",
      "Generated function derivative by x1 at point (10, 5, -2): -0.01\n",
      "Generated function derivative by x2 at point (10, 5, -2): -1.68\n",
      "Generated function derivative by x3 at point (10, 5, -2): -0.03\n",
      "\n",
      "numerical diff dz/dx1 ~= -0.01 (full form: -0.005588304363701723)\n",
      "numerical diff dz/dx2 ~= -1.68 (full form: -1.6814899234662837)\n",
      "numerical diff dz/dx3 ~= -0.03 (full form: -0.027941549818333297)\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.10.8 64-bit ('venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "interpreter": {
   "hash": "c297fa2a745451b05900aaa1d8e67bdcb601788b26d2968b81400e8972094f8b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}