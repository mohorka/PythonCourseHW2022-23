{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Задание 1\n",
    "\n",
    "(**NB.** для запуска примеров кода нужен Python версии не ниже **3.10**, допускается использование других версий, в этом случае нужно самостоятельно избавиться от конструкции `match`).\n",
    "\n",
    "Есть следующий код для [автоматического дифференцирования](https://en.wikipedia.org/wiki/Automatic_differentiation), в котором используются особенности системы типов языка `Python`: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Union, Callable\n",
    "from numbers import Number\n",
    "\n",
    "@dataclass(slots=True) #improved\n",
    "class Dual:\n",
    "    value: float\n",
    "    d: float\n",
    "\n",
    "    def __add__(self, other: Union[\"Dual\", Number]) -> \"Dual\":\n",
    "         match other:\n",
    "            case Dual(o_value, o_d):\n",
    "                return Dual(self.value + o_value, self.d + o_d)\n",
    "            case Number():\n",
    "                return Dual(float(other) + self.value, self.d)\n",
    "\n",
    "    def __mul__(self, other: Union[\"Dual\", Number]) -> \"Dual\":\n",
    "         match other:\n",
    "            case Dual(o_value, o_d):\n",
    "                return Dual(self.value * o_value, self.value * o_d + self.d * o_value)\n",
    "            case Number():\n",
    "                return Dual(float(other) * self.value, float(other) * self.d)    \n",
    "\n",
    "    __rmul__ = __mul__  # https://docs.python.org/3/reference/datamodel.html#object.__mul__\n",
    "    __radd__ = __add__  # https://docs.python.org/3/reference/datamodel.html#object.__radd__\n",
    " \n",
    "\n",
    "def diff(func: Callable[[float], float]) -> Callable[[float], float]:\n",
    "    return lambda x: func(Dual(x, 1.0)).d "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Поддерживаются две операции - сложение и умножение. Применить можно так:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Функция, которую будем дифференцировать\n",
    "def f(x: float) -> float:\n",
    "    return 5 * x * x + 2 * x + 2\n",
    "\n",
    "f_diff = diff(f)\n",
    "\n",
    "# значение производной в точке x = 2\n",
    "f_diff(2)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "22.0"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Задание 1.1 (5 баллов)\n",
    "\n",
    "Какие недостатки вы видите в данной реализации? Реализуйте поддержку (полностью самостоятельно или модифицируя приведенный код):\n",
    "- [унарных операций](https://docs.python.org/3/reference/datamodel.html#object.__neg__) \n",
    "- деления\n",
    "- возведения в степень\n",
    "\n",
    "Каким образом можно проверить корректность решения?  Реализуйте достаточный, по вашему мнению, набор тестов."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Задание 1.2 (7 баллов)\n",
    "Придумайте способ и реализуйте поддержку функций:\n",
    "- `exp()`\n",
    "- `cos()`\n",
    "- `sin()`\n",
    "- `log()`\n",
    "\n",
    "Добавьте соответствующие тесты"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Задание 1.3 (3 балла)\n",
    "\n",
    "Воспользуйтесь методами **численного** дифференцирования для \"проверки\" работы кода на нескольких примерах. Например,  библиотеке `scipy` есть функция `derivative`. Или реализуйте какой-нибудь метод численного дифференцирования самостоятельно (**+5 баллов**)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from scipy.misc import derivative\n",
    "\n",
    "def f(x: float) -> float:\n",
    "    return 5 * x * x + 2 * x + 2\n",
    "\n",
    "derivative(f, 2.)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "22.0"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Задание 1.4 (10 баллов)\n",
    "\n",
    "Необходимо разработать систему автоматического тестирования алгоритма дифференцирования в следующем виде:\n",
    "- реализовать механизм генерации \"случайных функций\" (например, что-то вроде такого: $f(x) = x + 5 * x - \\cos(20 * \\log(12 - 20 * x * x )) - 20 * x$ )\n",
    "- сгенерировать достаточно большое число функций и сравнить результаты символьного и численного дифференцирования в случайных точках "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Генерацию случайных функций можно осуществить, например, двумя путями. \n",
    "1. Генерировать функцию в текстовом виде, зачем использовать встроенную функцию [eval](https://docs.python.org/3/library/functions.html#eval)\n",
    "\n",
    "```python\n",
    "func = eval(\"lambda x: 2 * x + 5\")\n",
    "assert func(42) == 89 \n",
    "```\n",
    "\n",
    "2. Использовать стандартный модуль [ast](https://docs.python.org/3/library/ast.html), который позволяет во время выполнения программы манипулировать [Абстрактным Синтаксическим Деревом](https://ru.wikipedia.org/wiki/%D0%90%D0%B1%D1%81%D1%82%D1%80%D0%B0%D0%BA%D1%82%D0%BD%D0%BE%D0%B5_%D1%81%D0%B8%D0%BD%D1%82%D0%B0%D0%BA%D1%81%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B5_%D0%B4%D0%B5%D1%80%D0%B5%D0%B2%D0%BE).\n",
    "Например, выражение \n",
    "\n",
    "```python\n",
    "func = lambda x: 2 * x + 5\n",
    "```\n",
    "\n",
    "Можно запрограммировать с помощью кода:\n",
    "\n",
    "```python\n",
    "\n",
    "expr = ast.Expression(\n",
    "    body=ast.Lambda(\n",
    "        args=ast.arguments(\n",
    "            args=[\n",
    "                ast.arg(arg='x')\n",
    "            ],\n",
    "            posonlyargs=[],\n",
    "            kwonlyargs=[],\n",
    "            kw_defaults=[],\n",
    "            defaults=[]\n",
    "        ),\n",
    "        body=ast.BinOp(\n",
    "            left=ast.BinOp(\n",
    "                left=ast.Constant(value=2),\n",
    "                op=ast.Mult(),\n",
    "                right=ast.Name(id='x', ctx=ast.Load())\n",
    "            ),\n",
    "            op=ast.Add(),\n",
    "            right=ast.Constant(value=5)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "ast.fix_missing_locations(expr)\n",
    "\n",
    "func = eval(compile(expr, filename=\"\", mode=\"eval\"))\n",
    "\n",
    "assert func(42) == 89\n",
    "```\n",
    "\n",
    "При реализации нужно учитывать области допустимых значений функций."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Задание 1.5 (7 баллов)\n",
    "\n",
    "Реализуйте поддержку функций нескольких аргументов. Например\n",
    "\n",
    "```python\n",
    "def f(x: float, y: float, z: float) -> float:\n",
    "    return x * y + z - 5 * y  \n",
    "\n",
    "\n",
    "f_diff = diff(f)\n",
    "\n",
    "f_diff(10, 10, 10) # = [10, 5, 1]\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's take a look at the task as a computational graph (what it actually is) ~~say 'hello' to TensorFlow~~. Code beneath covers tasks 1.1, 1.2 and 1.5"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "source": [
    "import numpy as np\n",
    "import sympy\n",
    "import random\n",
    "from abc import ABC, abstractmethod"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "source": [
    "np.random.seed(2023)\n",
    "random.seed(2023)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "source": [
    "class Node(ABC):\n",
    "    \"\"\"Node of computational graph. Abstract class.\"\"\"    \n",
    "    @abstractmethod\n",
    "    def backward(self, var):\n",
    "        \"\"\"Create new node in graph.\n",
    "        \"\"\"        \n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def compute(self):\n",
    "        \"\"\"TBD.\"\"\"        \n",
    "        pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "source": [
    "class Const(Node):\n",
    "    \"\"\"Representates const type of node.\n",
    "    \"\"\"    \n",
    "    def __init__(self, value: float | int):\n",
    "        self.value = value\n",
    "    \n",
    "    def backward(self, var):\n",
    "        return Const(0.0)\n",
    "\n",
    "    def compute(self):\n",
    "        return self.value\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.value)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "source": [
    "class Variable(Node):\n",
    "    \"\"\"Representation of variable.\n",
    "    \"\"\"    \n",
    "    def __init__(self, name, value=None):\n",
    "        self.name = name\n",
    "        self.value = value\n",
    "    \n",
    "    def backward(self, var):\n",
    "        return Const(1) if self == var else Const(0)\n",
    "\n",
    "    def compute(self):\n",
    "        if self.value is None:\n",
    "            raise ValueError('variable seems to be empty')\n",
    "        return self.value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.name}'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "source": [
    "class Sum(Node):\n",
    "    \"\"\"Summary operation.\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y):\n",
    "        self.x, self.y = x, y\n",
    "    \n",
    "    def backward(self, var):\n",
    "        return Sum(self.x.backward(var), self.y.backward(var))\n",
    "\n",
    "    def compute(self):\n",
    "        return self.x.compute() + self.y.compute()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'({self.x} + {self.y})'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "source": [
    "class Mul(Node):\n",
    "    \"\"\"Multiply operation.\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y):\n",
    "        self.x, self.y = x, y\n",
    "    \n",
    "    def backward(self, var):\n",
    "        return Sum(\n",
    "            Mul(self.x.backward(var), self.y),\n",
    "            Mul(self.x, self.y.backward(var))\n",
    "        )\n",
    "\n",
    "    def compute(self):\n",
    "        return self.x.compute() * self.y.compute()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'({self.x} * {self.y})'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "source": [
    "class Neg(Node):\n",
    "    \"\"\"Change sign of Node to opposite.\n",
    "    x > 0 -> Neg(x) -> x < 0\n",
    "    \"\"\"\n",
    "    def __init__(self, x):\n",
    "        self.x =  x\n",
    "    \n",
    "    def backward(self, var):\n",
    "        return Neg(self.x.backward(var))\n",
    "    \n",
    "    def compute(self):\n",
    "        return - self.x.compute()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'(-{self.x})'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "source": [
    "class Power(Node):\n",
    "    \"\"\"Power operation.\n",
    "    Notice, that first argument raises to power from second argument.\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y):\n",
    "        self.x, self.y = x, y\n",
    "    \n",
    "    def backward(self, var):\n",
    "        return Sum(\n",
    "            Mul(Mul(self.y, Power(self.x, Sum(self.y, Const(-1)))), self.x.backward(var)),\n",
    "            Mul(Mul(Power(self.x, self.y), self.y.backward(var)), Log(self.x))\n",
    "        )\n",
    "    \n",
    "    def compute(self):\n",
    "        return self.x.compute() ** self.y.compute()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'({self.x} ** {self.y})'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "source": [
    "class Divide(Node):\n",
    "    \"\"\"Divide first argument to the second.\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y):\n",
    "        self.x, self.y = x, y\n",
    "    \n",
    "    def backward(self, var):\n",
    "        return Divide(\n",
    "            Sum(\n",
    "                Mul(self.x.backward(var), self.y),\n",
    "                Neg(Mul(self.y.backward(var), self.x))\n",
    "            ),\n",
    "            Power(self.y, Const(2))\n",
    "        )\n",
    "    \n",
    "    def compute(self):\n",
    "        return self.x.compute() / self.y.compute()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'({self.x} / {self.y})'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "source": [
    "class Abs(Node):\n",
    "    \"\"\"Represent absolute operation. \n",
    "    Notice that abs(x) -- not smooth-function, so it may have some unexpected side-effects 🤓.\\n\n",
    "    This implementaion is based on assumption that if submodule expression is lower than zero,\n",
    "    then it's diff and value change their signs. Otherwise nothing happens.\n",
    "    \"\"\"\n",
    "    def __init__(self, x):\n",
    "        self.x =  x\n",
    "    \n",
    "    def backward(self, var):\n",
    "        return Abs(self.x.backward(var))\n",
    "    \n",
    "    def compute(self):\n",
    "        value = self.x.compute()\n",
    "        if value == 0:\n",
    "            return 0\n",
    "        return value if value > 0 else -value\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'(|{self.x}|)'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "source": [
    "x = Variable('x', 3)\n",
    "y = Variable('y', 2)\n",
    "\n",
    "z = Abs(Sum(y, Neg(x)))\n",
    "\n",
    "z\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(|(y + (-x))|)"
      ]
     },
     "metadata": {},
     "execution_count": 347
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "source": [
    "z.backward(x)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(|(0 + (-1))|)"
      ]
     },
     "metadata": {},
     "execution_count": 348
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "source": [
    "z.backward(x).compute()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 349
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "source": [
    "#x * y + z - 5 * y  \n",
    "x = Variable('x', 10)\n",
    "y = Variable('y', 10)\n",
    "z = Variable('z', 10)\n",
    "\n",
    "f = Sum(\n",
    "    Sum(\n",
    "        Mul(x,y),\n",
    "        z \n",
    "    ), Neg(Mul(Const(5),y))\n",
    ")\n",
    "f"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(((x * y) + z) + (-(5 * y)))"
      ]
     },
     "metadata": {},
     "execution_count": 350
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "source": [
    "f.backward(x).compute()\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "metadata": {},
     "execution_count": 351
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "source": [
    "x = Variable('x', 3)\n",
    "y = Variable('y', 2)\n",
    "z = Power(x,y)\n",
    "z"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(x ** y)"
      ]
     },
     "metadata": {},
     "execution_count": 352
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "source": [
    "z.compute()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "metadata": {},
     "execution_count": 353
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "source": [
    "z.backward(y)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(((y * (x ** (y + -1))) * 0) + (((x ** y) * 1) * log(x)))"
      ]
     },
     "metadata": {},
     "execution_count": 354
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "source": [
    "z.backward(y).compute()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9.887510598012987"
      ]
     },
     "metadata": {},
     "execution_count": 355
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's make our toy auto diff easier to use: add symbolic operations. To not-built-in functions add our mixin to class directly:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "source": [
    "from typing import Any\n",
    "class AutoGradMixin:\n",
    "    \"\"\"Mixin for covering all nodes with symbolic operators.\n",
    "    \"\"\"    \n",
    "    @staticmethod\n",
    "    def _to_node(x: Node | Any):\n",
    "        \"\"\"check if 'x' is Node, otherwise create Const object base on 'x'\n",
    "\n",
    "        Args:\n",
    "            x (Node | Any): Node or basement for a new Const\n",
    "        \"\"\"\n",
    "        return x if isinstance(x, Node) else Const(x) \n",
    "\n",
    "    def __add__(self, other):\n",
    "        return SymbolicSum(self, self._to_node(other))\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return SymbolicSum(self._to_node(other), self)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return SymbolicMul(self, self._to_node(other))     \n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return SymbolicMul(self._to_node(other), self)   \n",
    "\n",
    "    def __neg__(self):\n",
    "        return SymbolicNeg(self)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return SymbolicSum(self, SymbolicNeg(self._to_node(other)))\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        return SymbolicSum(self._to_node(other), SymbolicNeg(self))\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        return SymbolicPower(self, self._to_node(other))\n",
    "    \n",
    "    def __rpow__(self, other):\n",
    "        return SymbolicPower(self._to_node(other), self)\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return SymbolicDivide(self, self._to_node(other))\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        return SymbolicDivide(self._to_node(other), self)\n",
    "    \n",
    "    def __abs__(self):\n",
    "        return Abs(self)\n",
    "\n",
    "class SymbolicVar(Variable, AutoGradMixin):\n",
    "    pass\n",
    "\n",
    "class SymbolicConst(Const, AutoGradMixin):\n",
    "    pass\n",
    "\n",
    "class SymbolicSum(Sum, AutoGradMixin):\n",
    "    pass\n",
    "\n",
    "class SymbolicMul(Mul, AutoGradMixin):\n",
    "    pass\n",
    "\n",
    "class SymbolicNeg(Neg, AutoGradMixin):\n",
    "    pass\n",
    "\n",
    "class SymbolicPower(Power, AutoGradMixin):\n",
    "    pass\n",
    "\n",
    "class SymbolicDivide(Divide, AutoGradMixin):\n",
    "    pass\n",
    "\n",
    "class Abs(Node, AutoGradMixin):\n",
    "    \"\"\"Represent absolute operation. \n",
    "    Notice that abs(x) -- not smooth-function, so it may have some unexpected side-effects 🤓.\\n\n",
    "    This implementaion is based on assumption that if submodule expression is lower than zero,\n",
    "    then it's diff and value change their signs. Otherwise nothing happens.\n",
    "    \"\"\"\n",
    "    def __init__(self, x):\n",
    "        self.x =  x\n",
    "    \n",
    "    def backward(self, var):\n",
    "        return Abs(self.x.backward(var))\n",
    "    \n",
    "    def compute(self):\n",
    "        value = self.x.compute()\n",
    "        if value == 0:\n",
    "            return 0\n",
    "        return value if value > 0 else -value\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'(|{self.x}|)'\n",
    "\n",
    "class Log(Node, AutoGradMixin):\n",
    "    \"\"\"Represent natural logarithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, x) -> None:\n",
    "        self.x = x\n",
    "    \n",
    "    def backward(self, var):\n",
    "        return Mul(\n",
    "            Divide(Const(1), self.x),\n",
    "            self.x.backward(var)\n",
    "            )\n",
    "    \n",
    "    def compute(self):\n",
    "        return np.log(self.x.compute())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'log({self.x})'\n",
    "\n",
    "class Exp(Node, AutoGradMixin):\n",
    "    \"\"\"Represent exponential operation\n",
    "    \"\"\"\n",
    "    def __init__(self, x) -> None:\n",
    "        self.x = x\n",
    "    \n",
    "    def backward(self, var):\n",
    "        return Mul(Exp(self.x), self.x.backward(var))\n",
    "    \n",
    "    def compute(self):\n",
    "        return np.exp(self.x.compute())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'e^({self.x})'\n",
    "\n",
    "class Cos(Node, AutoGradMixin):\n",
    "    \"\"\"Represent cosine operation\n",
    "    \"\"\"\n",
    "    def __init__(self, x) -> None:\n",
    "        self.x = x\n",
    "    \n",
    "    def backward(self, var):\n",
    "        return Mul(Neg(Sin(self.x)), self.x.backward(var))\n",
    "    \n",
    "    def compute(self):\n",
    "        return np.cos(self.x.compute())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'cos({self.x})'\n",
    "\n",
    "class Sin(Node, AutoGradMixin):\n",
    "    \"\"\"Represent sinus operation\n",
    "    \"\"\"\n",
    "    def __init__(self, x) -> None:\n",
    "        self.x = x\n",
    "    \n",
    "    def backward(self, var):\n",
    "        return Mul(Cos(self.x), self.x.backward(var))\n",
    "    \n",
    "    def compute(self):\n",
    "        return np.sin(self.x.compute())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'sin({self.x})'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing time! 🛠\n",
    "\n",
    "Let's use just simple \"asserts\" with sympy for this moment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "source": [
    "#testing binary ops, division and powering\n",
    "x_value, y_value = np.random.randint(1,100), np.random.randint(1,10)\n",
    "x = SymbolicVar('x', x_value)\n",
    "y = SymbolicVar('y', y_value)\n",
    "z1 = x * x + x * y * 5 + 4\n",
    "z1_diff_x = z1.backward(x).compute()\n",
    "z1_diff_y = z1.backward(y).compute()\n",
    "\n",
    "z2 = x ** y\n",
    "z2_diff_x = z2.backward(x).compute()\n",
    "z2_diff_y = z2.backward(y).compute()\n",
    "\n",
    "z3 = (3 * x) / y\n",
    "z3_diff_x = z3.backward(x).compute()\n",
    "z3_diff_y = z3.backward(y).compute()\n",
    "\n",
    "#sympy setup\n",
    "x = sympy.Symbol('x')\n",
    "y = sympy.Symbol('y')\n",
    "z1 = x * x + x * y * 5 + 4\n",
    "assert sympy.lambdify([x,y], sympy.diff(z1, x))(x_value,y_value) == z1_diff_x\n",
    "assert sympy.lambdify([x,y], sympy.diff(z1, y))(x_value,y_value) == z1_diff_y\n",
    "\n",
    "z2 = x ** y\n",
    "assert sympy.lambdify([x,y], sympy.diff(z2, x))(x_value,y_value) == z2_diff_x\n",
    "assert sympy.lambdify([x,y], sympy.diff(z2, y))(x_value,y_value) == z2_diff_y\n",
    "\n",
    "z3 = (3 * x) / y\n",
    "assert sympy.lambdify([x,y], sympy.diff(z3, x))(x_value,y_value) == z3_diff_x\n",
    "assert sympy.lambdify([x,y], sympy.diff(z3, y))(x_value,y_value) == z3_diff_y\n",
    "print(\"Passed  🎉\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Passed  🎉\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "source": [
    "#testing exp, cos, sin, log\n",
    "x_value, y_value = np.random.randint(1,100), np.random.randint(1,10)\n",
    "x = SymbolicVar('x', x_value)\n",
    "y = SymbolicVar('y', y_value)\n",
    "z1 = Exp(x + y)\n",
    "z1_diff_x = z1.backward(x).compute()\n",
    "z1_diff_y = z1.backward(y).compute()\n",
    "\n",
    "z2 = Cos(x - y)\n",
    "z2_diff_x = z2.backward(x).compute()\n",
    "z2_diff_y = z2.backward(y).compute()\n",
    "\n",
    "z3 = Sin(x * y)\n",
    "z3_diff_x = z3.backward(x).compute()\n",
    "z3_diff_y = z3.backward(y).compute()\n",
    "\n",
    "z4 = Log(Sin(x) + Cos(y))\n",
    "z4_diff_x = z4.backward(x).compute()\n",
    "z4_diff_y = z4.backward(y).compute()\n",
    "\n",
    "#sympy setup\n",
    "x = sympy.Symbol('x')\n",
    "y = sympy.Symbol('y')\n",
    "z1 = sympy.exp(x + y)\n",
    "assert sympy.lambdify([x,y], sympy.diff(z1, x))(x_value,y_value) == z1_diff_x\n",
    "assert sympy.lambdify([x,y], sympy.diff(z1, y))(x_value,y_value) == z1_diff_y\n",
    "\n",
    "z2 = sympy.cos(x - y)\n",
    "assert sympy.lambdify([x,y], sympy.diff(z2, x))(x_value,y_value) == z2_diff_x\n",
    "assert sympy.lambdify([x,y], sympy.diff(z2, y))(x_value,y_value) == z2_diff_y\n",
    "\n",
    "z3 = sympy.sin(x * y)\n",
    "assert sympy.lambdify([x,y], sympy.diff(z3, x))(x_value,y_value) == z3_diff_x\n",
    "assert sympy.lambdify([x,y], sympy.diff(z3, y))(x_value,y_value) == z3_diff_y\n",
    "\n",
    "z4 = sympy.log(sympy.sin(x) + sympy.cos(y))\n",
    "assert sympy.lambdify([x,y], sympy.diff(z4, x))(x_value,y_value) == z4_diff_x\n",
    "assert sympy.lambdify([x,y], sympy.diff(z4, y))(x_value,y_value) == z4_diff_y\n",
    "print(\"Passed  🎉\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Passed  🎉\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Almost here 🎉 Two steps left: implement own numerical differentation mechanism & generator of testing functions. Let's start with first one (covers 1.3 task).\n",
    "\n",
    "**Newton's difference quotient** -- according to [wiki](https://en.wikipedia.org/wiki/Numerical_differentiation), our diff can be represented as finite difference approximations.\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "source": [
    "def partial_difference_quotient(f, var, i, step):\n",
    "    \"\"\"compute the i-th partial difference quotient of function f at var\"\"\"\n",
    "    var_step = [var_j + (step if j == i else 0) for j, var_j in enumerate(var)]\n",
    "\n",
    "    return (f(var_step) - f(var)) / step #slope\n",
    "\n",
    "def estimate_gradient(f, var, step=0.00001):\n",
    "    return [partial_difference_quotient(f, var, i, step) for i, _ in enumerate(var)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "source": [
    "x_value, y_value = np.random.randint(1,100), np.random.randint(1,10)\n",
    "print(f\"x: {x_value}, y: {y_value}\")\n",
    "x = SymbolicVar('x', x_value)\n",
    "y = SymbolicVar('y', y_value)\n",
    "z = x * x + x * y * 3 + 1 \n",
    "print(f\"autograd dz/dx = {z.backward(x).compute()}\")\n",
    "print(f\"autograd dz/dy = {z.backward(y).compute()}\")\n",
    "\n",
    "def f(v):\n",
    "    x, y = v\n",
    "    return x * x + x * y * 3 + 1 \n",
    "\n",
    "var = [x_value, y_value]\n",
    "grad = estimate_gradient(f, var, step = 0.00001)\n",
    "formatted_grad = [ '%.2f' % elem for elem in grad ]\n",
    "print(f\"numerical diff dz/dx ~= {formatted_grad[0]} (full form: {grad[0]})\")\n",
    "print(f\"numerical diff dz/dy ~= {formatted_grad[1]} (full form: {grad[1]})\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x: 93, y: 4\n",
      "autograd dz/dx = 198.0\n",
      "autograd dz/dy = 279.0\n",
      "numerical diff dz/dx ~= 198.00 (full form: 198.0000100957113)\n",
      "numerical diff dz/dy ~= 279.00 (full form: 279.00000004592584)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "source": [
    "x_value, y_value = np.random.randint(1,100), np.random.randint(1,10)\n",
    "print(f\"x: {x_value}, y: {y_value}\")\n",
    "x = SymbolicVar('x', x_value)\n",
    "y = SymbolicVar('y', y_value)\n",
    "z = Abs(x-y)\n",
    "print(f\"autograd dz/dx = {z.backward(x).compute()}\")\n",
    "print(f\"autograd dz/dy = {z.backward(y).compute()}\")\n",
    "\n",
    "def f(v):\n",
    "    x, y = v\n",
    "    return abs(x - y)\n",
    "\n",
    "var = [x_value, y_value]\n",
    "grad = estimate_gradient(f, var, step = 0.00001)\n",
    "formatted_grad = [ '%.2f' % elem for elem in grad ]\n",
    "print(f\"numerical diff dz/dx ~= {formatted_grad[0]} (full form: {grad[0]})\")\n",
    "print(f\"numerical diff dz/dy ~= {formatted_grad[1]} (full form: {grad[1]})\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x: 53, y: 7\n",
      "autograd dz/dx = 1\n",
      "autograd dz/dy = 1\n",
      "numerical diff dz/dx ~= 1.00 (full form: 1.0000000003174137)\n",
      "numerical diff dz/dy ~= -1.00 (full form: -1.0000000003174137)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, pretty close to our auto-diff's results! And last but now least step: let's create toy functions' generator.\n",
    "\n",
    "It would be nice to test if result of auto-diff of generated function is the same as result of numerical diff of generated function, so we will keep this idea in mind and write method which generates mathematically the same function but in two different forms - as an input for auto-diff and as an input for numerical one.\n",
    "\n",
    "Additionally, let's fully cover task 1.5 by allow to generate multivariable functions\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "source": [
    "import ast\n",
    "import math\n",
    "from typing import Callable"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "source": [
    "def _arg_name(i: int):\n",
    "    return f\"x{i}\"\n",
    "\n",
    "def _random_expr(args_amount: int, recursion_limit: int):\n",
    "    recursion_limit -= 1\n",
    "\n",
    "    if recursion_limit == 1:\n",
    "        unary_node_without_recursive_calls_type = random.choice([\n",
    "            'const',\n",
    "            'var',\n",
    "        ])\n",
    "        if unary_node_without_recursive_calls_type == 'const':\n",
    "            value = random.randint(-10, 10)\n",
    "            return [\n",
    "                ast.Call(ast.Name(id='SymbolicConst', ctx=ast.Load()), args=[ast.Constant(value)], keywords=[]),\n",
    "                ast.Constant(value)\n",
    "            ] \n",
    "        elif unary_node_without_recursive_calls_type == 'var':\n",
    "            name = _arg_name(random.randint(0,args_amount-1))\n",
    "            return [ast.Name(id=name, ctx=ast.Load()), ast.Name(id=name, ctx=ast.Load())]\n",
    "\n",
    "\n",
    "    # choose binary or unary node\n",
    "    if random.random() < 0.4:\n",
    "        # expression will be bin op\n",
    "        left = _random_expr(args_amount,recursion_limit)\n",
    "        right = _random_expr(args_amount, recursion_limit)\n",
    "        op = random.choice([ast.Add(), ast.Sub(), ast.Mult(), ast.Div(), ast.Pow()])\n",
    "        return [\n",
    "            ast.BinOp(\n",
    "                left=left[0],\n",
    "                op=op,\n",
    "                right=right[0]\n",
    "            ),\n",
    "            ast.BinOp(\n",
    "                left=left[1],\n",
    "                op=op,\n",
    "                right=right[1]\n",
    "            )\n",
    "        ]\n",
    "    else:\n",
    "        # expression will be single node\n",
    "        unary_node_type = random.choice([\n",
    "            'const',\n",
    "            'var',\n",
    "            'abs',\n",
    "            'cos',\n",
    "            'sin',\n",
    "            'exp',\n",
    "            'log',\n",
    "        ])\n",
    "        if unary_node_type == 'const':\n",
    "            value = random.randint(-10, 10)\n",
    "            return [\n",
    "                ast.Call(ast.Name(id='SymbolicConst', ctx=ast.Load()), args=[ast.Constant(value)], keywords=[]),\n",
    "                ast.Constant(value)\n",
    "            ]\n",
    "        if unary_node_type == 'var':\n",
    "            name = _arg_name(random.randint(0,args_amount-1))\n",
    "            return [ast.Name(id=name, ctx=ast.Load()), ast.Name(id=name, ctx=ast.Load())]\n",
    "        elif unary_node_type == 'abs':\n",
    "            expr = _random_expr(args_amount, recursion_limit)\n",
    "            return [\n",
    "                ast.Call(ast.Name(id='Abs', ctx=ast.Load()), args=[expr[0]], keywords=[]),\n",
    "                ast.Call(ast.Name(id='abs', ctx=ast.Load()), args=[expr[1]], keywords=[])\n",
    "            ]\n",
    "        elif unary_node_type == 'cos':\n",
    "            expr = _random_expr(args_amount, recursion_limit)\n",
    "            return [\n",
    "                ast.Call(ast.Name(id='Cos', ctx=ast.Load()), args=[expr[0]], keywords=[]),\n",
    "                ast.Call(ast.Name(id='cos', ctx=ast.Load()), args=[expr[1]], keywords=[])\n",
    "            ]\n",
    "        elif unary_node_type == 'sin':\n",
    "            expr = _random_expr(args_amount, recursion_limit)\n",
    "            return [\n",
    "                ast.Call(ast.Name(id='Sin', ctx=ast.Load()), args=[expr[0]], keywords=[]),\n",
    "                ast.Call(ast.Name(id='sin', ctx=ast.Load()), args=[expr[1]], keywords=[])\n",
    "            ]\n",
    "        elif unary_node_type == 'log':\n",
    "            expr = _random_expr(args_amount, recursion_limit)\n",
    "            return [\n",
    "                ast.Call(ast.Name(id='Log', ctx=ast.Load()), args=[expr[0]], keywords=[]),\n",
    "                ast.Call(ast.Name(id='log', ctx=ast.Load()), args=[expr[1]], keywords=[])\n",
    "            ]\n",
    "        elif unary_node_type == 'exp':\n",
    "            expr = _random_expr(args_amount, recursion_limit)\n",
    "            return [\n",
    "                ast.Call(ast.Name(id='Exp', ctx=ast.Load()), args=[expr[0]], keywords=[]),\n",
    "                ast.Call(ast.Name(id='exp', ctx=ast.Load()), args=[expr[1]], keywords=[])\n",
    "            ]\n",
    "\n",
    "\n",
    "def generate_function(args_amount: int = 1, recursion_limit: int = 10, verbose=True) -> Callable[[float], float]:\n",
    "    body = _random_expr(args_amount, recursion_limit)\n",
    "    expr = ast.Expression(\n",
    "        body=ast.Lambda(\n",
    "            args=ast.arguments(\n",
    "                args=[\n",
    "                    ast.arg(arg=_arg_name(i))\n",
    "                    for i in range(args_amount)\n",
    "                ],\n",
    "                posonlyargs=[],\n",
    "                kwonlyargs=[],\n",
    "                kw_defaults=[],\n",
    "                defaults=[]\n",
    "            ),\n",
    "            body=body[0]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    expr_for_num_diff = ast.Expression(\n",
    "        body=ast.Lambda(\n",
    "            args=ast.arguments(\n",
    "                args=[\n",
    "                    ast.arg(arg=_arg_name(i))\n",
    "                    for i in range(args_amount)\n",
    "                ],\n",
    "                posonlyargs=[],\n",
    "                kwonlyargs=[],\n",
    "                kw_defaults=[],\n",
    "                defaults=[]\n",
    "            ),\n",
    "            body=body[1]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    ast.fix_missing_locations(expr)\n",
    "    ast.fix_missing_locations(expr_for_num_diff)\n",
    "    if verbose:\n",
    "        print(f\"Generated function for auto-diff: {ast.unparse(expr)}\")\n",
    "        print(f\"Generated function for numerical diff: {ast.unparse(expr_for_num_diff)}\\n\")\n",
    "    compiled_func = compile(expr, filename=\"\", mode=\"eval\")\n",
    "    compiled_func_for_num_diff = compile(expr_for_num_diff, filename=\"\", mode=\"eval\")\n",
    "    func = eval(compiled_func, {'Abs': Abs, 'Cos': Cos, 'Sin': Sin, 'Log': Log, 'Exp': Exp, 'SymbolicConst': SymbolicConst})\n",
    "    func_for_numerical_diff = eval(compiled_func_for_num_diff, {'abs':abs, 'cos': math.cos, 'sin': math.sin, 'log': math.log,'exp': math.exp})\n",
    "    return func, func_for_numerical_diff"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our toy auto-gen sometimes produces funny things, such as many stacked exponents which causes inf as a result; unfortunately, because we generate functions randomly, this kind of behaviour is necessary evil, just reload cell 🤓 (~~probably it can be fixed somehow at least author doesn't know how~~)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "But let's take a look at the result of all things below:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "source": [
    "x = SymbolicVar('x', 10)\n",
    "func, func4num = generate_function(recursion_limit=4)\n",
    "print(f\"Generated function: {func(x)}\")\n",
    "print(f\"Generated function at point {x.value}: {func(x).compute():.2f}\")\n",
    "print(f\"Generated function derivative: {func(x).backward(x)}\")\n",
    "print(f\"Generated function derivative at point {x.value}: {func(x).backward(x).compute():.2f}\\n\")\n",
    "\n",
    "def f(v):\n",
    "    x = v[0]\n",
    "    return func4num(x)\n",
    "\n",
    "var = [x.value]\n",
    "grad = estimate_gradient(f, var, step = 0.00001)\n",
    "print(f\"Generated function numerical derivative at point {x.value}: {grad[0]:.2f}\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generated function for auto-diff: lambda x0: Cos(SymbolicConst(-7) + x0)\n",
      "Generated function for numerical diff: lambda x0: cos(-7 + x0)\n",
      "\n",
      "Generated function: cos((-7 + x))\n",
      "Generated function at point 10: -0.99\n",
      "Generated function derivative: ((-sin((-7 + x))) * (0.0 + 1))\n",
      "Generated function derivative at point 10: -0.14\n",
      "\n",
      "Generated function numerical derivative at point 10: -0.14\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "source": [
    "x = SymbolicVar('x', 10)\n",
    "y = SymbolicVar('y', 5)\n",
    "func, func4num = generate_function(args_amount=2, recursion_limit=5)\n",
    "print(f\"Generated function: {func(x, y)}\")\n",
    "print(f\"Generated function at point ({x.value}, {y.value}): {func(x, y).compute():.2f}\")\n",
    "print(f\"Generated function derivative by x: {func(x,y).backward(x)}\")\n",
    "print(f\"Generated function derivative by y: {func(x,y).backward(y)}\")\n",
    "print(f\"Generated function derivative by x at point ({x.value}, {y.value}): {func(x,y).backward(x).compute():.2f}\")\n",
    "print(f\"Generated function derivative by y at point ({x.value}, {y.value}): {func(x,y).backward(y).compute():.2f}\\n\")\n",
    "\n",
    "def f(v):\n",
    "    x, y = v\n",
    "    return func4num(x, y)\n",
    "\n",
    "var = [x.value, y.value]\n",
    "grad = estimate_gradient(f, var, step = 0.00001)\n",
    "formatted_grad = [ '%.2f' % elem for elem in grad ]\n",
    "print(f\"numerical diff dz/dx ~= {formatted_grad[0]} (full form: {grad[0]})\")\n",
    "print(f\"numerical diff dz/dy ~= {formatted_grad[1]} (full form: {grad[1]})\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generated function for auto-diff: lambda x0, x1: Abs(Cos(x1))\n",
      "Generated function for numerical diff: lambda x0, x1: abs(cos(x1))\n",
      "\n",
      "Generated function: (|cos(y)|)\n",
      "Generated function at point (10, 5): 0.28\n",
      "Generated function derivative by x: (|((-sin(y)) * 0)|)\n",
      "Generated function derivative by y: (|((-sin(y)) * 1)|)\n",
      "Generated function derivative by x at point (10, 5): 0.00\n",
      "Generated function derivative by y at point (10, 5): 0.96\n",
      "\n",
      "numerical diff dz/dx ~= 0.00 (full form: 0.0)\n",
      "numerical diff dz/dy ~= 0.96 (full form: 0.9589228563033901)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "source": [
    "x1 = SymbolicVar('x1', 10)\n",
    "x2 = SymbolicVar('x2', 5)\n",
    "x3 = SymbolicVar('x3', -2)\n",
    "func, func4num = generate_function(args_amount=3, recursion_limit=5)\n",
    "print(f\"Generated function: {func(x1, x2, x3)}\")\n",
    "print(f\"Generated function at point ({x1.value}, {x2.value}, {x3.value}): {func(x1, x2, x3).compute():.2f}\\n\")\n",
    "print(f\"Generated function derivative by x1: {func(x1, x2, x3).backward(x1)}\")\n",
    "print(f\"Generated function derivative by x2: {func(x1, x2, x3).backward(x2)}\")\n",
    "print(f\"Generated function derivative by x3: {func(x1, x2, x3).backward(x3)}\\n\")\n",
    "print(f\"Generated function derivative by x1 at point ({x1.value}, {x2.value}, {x3.value}): {func(x1, x2, x3).backward(x1).compute():.2f}\")\n",
    "print(f\"Generated function derivative by x2 at point ({x1.value}, {x2.value}, {x3.value}): {func(x1, x2, x3).backward(x2).compute():.2f}\")\n",
    "print(f\"Generated function derivative by x3 at point ({x1.value}, {x2.value}, {x3.value}): {func(x1, x2, x3).backward(x3).compute():.2f}\\n\")\n",
    "\n",
    "def f(v):\n",
    "    x1, x2, x3 = v\n",
    "    return func4num(x1, x2, x3)\n",
    "\n",
    "var = [x1.value, x2.value, x3.value]\n",
    "grad = estimate_gradient(f, var, step = 0.00001)\n",
    "formatted_grad = [ '%.2f' % elem for elem in grad ]\n",
    "print(f\"numerical diff dz/dx1 ~= {formatted_grad[0]} (full form: {grad[0]})\")\n",
    "print(f\"numerical diff dz/dx2 ~= {formatted_grad[1]} (full form: {grad[1]})\")\n",
    "print(f\"numerical diff dz/dx3 ~= {formatted_grad[2]} (full form: {grad[2]})\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generated function for auto-diff: lambda x0, x1, x2: (x2 / x0 - Log(x1)) * Sin(x1 - SymbolicConst(-1))\n",
      "Generated function for numerical diff: lambda x0, x1, x2: (x2 / x0 - log(x1)) * sin(x1 - -1)\n",
      "\n",
      "Generated function: (((x3 / x1) + (-log(x2))) * sin((x2 + (--1))))\n",
      "Generated function at point (10, 5, -2): 0.51\n",
      "\n",
      "Generated function derivative by x1: ((((((0 * x1) + (-(1 * x3))) / (x1 ** 2)) + (-((1 / x2) * 0))) * sin((x2 + (--1)))) + (((x3 / x1) + (-log(x2))) * (cos((x2 + (--1))) * (0 + (-0.0)))))\n",
      "Generated function derivative by x2: ((((((0 * x1) + (-(0 * x3))) / (x1 ** 2)) + (-((1 / x2) * 1))) * sin((x2 + (--1)))) + (((x3 / x1) + (-log(x2))) * (cos((x2 + (--1))) * (1 + (-0.0)))))\n",
      "Generated function derivative by x3: ((((((1 * x1) + (-(0 * x3))) / (x1 ** 2)) + (-((1 / x2) * 0))) * sin((x2 + (--1)))) + (((x3 / x1) + (-log(x2))) * (cos((x2 + (--1))) * (0 + (-0.0)))))\n",
      "\n",
      "Generated function derivative by x1 at point (10, 5, -2): -0.01\n",
      "Generated function derivative by x2 at point (10, 5, -2): -1.68\n",
      "Generated function derivative by x3 at point (10, 5, -2): -0.03\n",
      "\n",
      "numerical diff dz/dx1 ~= -0.01 (full form: -0.005588304363701723)\n",
      "numerical diff dz/dx2 ~= -1.68 (full form: -1.6814899234662837)\n",
      "numerical diff dz/dx3 ~= -0.03 (full form: -0.027941549818333297)\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.10.8 64-bit ('venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "interpreter": {
   "hash": "c297fa2a745451b05900aaa1d8e67bdcb601788b26d2968b81400e8972094f8b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}